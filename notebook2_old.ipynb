{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "train=train.replace('\\n',' ')\n",
    "test=test.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlp:\n",
    "    #need to download from nltk - 'averaged_perceptron_tagger','wordnet','stopwords'\n",
    "    re = __import__('re')\n",
    "    pd = __import__('pandas')\n",
    "    nltk = __import__('nltk')\n",
    "    np = __import__('numpy')\n",
    "    \n",
    "    def __init__(self):\n",
    "        print('nlp V.0.1 \\nImported pandas,re,nltk,numpy packages')\n",
    "        \n",
    "    def clean_text_for_topic_modelling(self,text_column):\n",
    "        text_column=text_column.str.lower()\n",
    "#         text_column = [self.re.sub(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b','', x) for x in text_column]\n",
    "        text_column = [x.split() for x in text_column]\n",
    "        temp_corpus=[]\n",
    "        for tweet in text_column:\n",
    "            new_list=[w for w in tweet if len(w)>2]\n",
    "            temp_corpus.append(new_list)\n",
    "        return self.pd.Series(temp_corpus)\n",
    "    \n",
    "    def remove_stopwords_from_corpus(self,text_column,extra_stopwords_list=[]):\n",
    "        from nltk.corpus import stopwords\n",
    "        stops = set(stopwords.words('english')).union(extra_stopwords_list)\n",
    "        text_column=text_column.apply(lambda x: [w for w in x if w not in stops])\n",
    "        return text_column\n",
    "    \n",
    "    def stemm_corpus(self,text_column,stemmer='porter'):\n",
    "        if stemmer=='porter':\n",
    "            from nltk.stem import PorterStemmer\n",
    "            porter = PorterStemmer()\n",
    "            text_column=text_column.apply(lambda x:[porter.stem(w) for w in x])\n",
    "        elif stemmer=='lancaster':\n",
    "            from nltk.stem import LancasterStemmer\n",
    "            lancaster=LancasterStemmer()\n",
    "            text_column=text_column.apply(lambda x:[lancaster.stem(w) for w in x])\n",
    "        else:\n",
    "            from nltk.stem import SnowballStemmer\n",
    "            snowball=SnowballStemmer(\"english\")\n",
    "            text_column=text_column.apply(lambda x:[snowball.stem(w) for w in x])\n",
    "        return text_column\n",
    "    \n",
    "    def lemmatize_corpus(self,text_column):\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        from nltk.corpus import wordnet\n",
    "        \n",
    "        def get_word_pos(word):\n",
    "            tag = self.nltk.pos_tag(word)[0][1][0].upper()\n",
    "            tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "            return tag_dict.get(tag, wordnet.NOUN)\n",
    "        \n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        text_column=text_column.apply(lambda x:[wordnet_lemmatizer.lemmatize(w, pos=get_word_pos(w)) for w in x])\n",
    "        return text_column\n",
    "    \n",
    "    def create_ngrams(self,text_column,replace=False,num_grams=2):\n",
    "        from nltk import ngrams\n",
    "        if replace:\n",
    "            grams_list=[]\n",
    "            for tweet in text_column:\n",
    "                grams_list.append([' '.join(ngram) for ngram in ngrams(tweet,num_grams)])\n",
    "            return self.pd.Series(grams_list)\n",
    "        else:\n",
    "            for i,tweet in text_column.iteritems():\n",
    "                copy_tweet=tweet.copy()\n",
    "                for ngram in ngrams(tweet,2):\n",
    "                    copy_tweet.append(' '.join(ngram))\n",
    "                text_column.at[i]=copy_tweet\n",
    "            return text_column\n",
    "        \n",
    "    def gensim_dic(self,text_column,filter_extremes=True,no_below=3,no_above=0.99):\n",
    "        from gensim.corpora import Dictionary\n",
    "        dictionary=Dictionary(text_column)\n",
    "        if filter_extremes:\n",
    "            dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "        return dictionary\n",
    "    \n",
    "    def bow_corpus(self,dictionary,text_column):\n",
    "        return [dictionary.doc2bow(doc) for doc in text_column]\n",
    "    \n",
    "    def one_hot_encoding_todf(self,dictionary,text_column):\n",
    "        from tqdm import tqdm\n",
    "        columns=[dictionary.get(i) for i in range(len(dictionary))]\n",
    "        one_hot_df=pd.DataFrame(columns=columns)\n",
    "        for i,row in tqdm(enumerate(train.comment_text)):\n",
    "            for word in row:\n",
    "                one_hot_df.at[i,word]=+1\n",
    "        return one_hot_df\n",
    "    \n",
    "    def tf_idf_tomatrix(self,dictionary,tf_idf_object):\n",
    "        from gensim.matutils import corpus2dense\n",
    "        num_terms = len(dictionary.keys())\n",
    "        num_docs = dictionary.num_docs\n",
    "        corpus_tfidf_dense = corpus2dense(tf_idf_object, num_terms, num_docs)\n",
    "        return self.pd.DataFrame(self.np.transpose(corpus_tfidf_dense))\n",
    "        \n",
    "    def vec_to_tfidf(self,bow_corpus):\n",
    "        from gensim import models\n",
    "        tfidf = models.TfidfModel(bow_corpus)\n",
    "        return tfidf[bow_corpus]\n",
    "    \n",
    "    def lda_model(self,tfidf_corp,dic,num_topics=20,passes=30,alpha=0.001,eta='auto'):\n",
    "        from gensim import models\n",
    "        return models.ldamodel.LdaModel(corpus=tfidf_corp, num_topics=num_topics, id2word=dic, passes=passes, alpha=alpha, \n",
    "                                             eta=eta,random_state=13)\n",
    "    \n",
    "    def get_coherence(self,lda_model,corpus,dic):\n",
    "        from gensim.models import CoherenceModel\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus, dictionary=dic, coherence='c_v')\n",
    "        return coherence_model_lda.get_coherence()\n",
    "    \n",
    "    def is_tweet_intopic(self,text_column,words_list):\n",
    "        \n",
    "        def is_tweets_words_inlist(tweet,words_list):\n",
    "            for word in tweet:\n",
    "                if word in words_list:\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        results=[]\n",
    "        for tweet in text_column:\n",
    "            results.append(is_tweets_words_inlist(tweet,words_list))\n",
    "        return self.pd.Series(results)\n",
    "    \n",
    "    def assign_lda_topic(self,lda_model,bow_corpus):\n",
    "        topics=[]\n",
    "        data=lda_model.get_document_topics(bow_corpus)\n",
    "        for tweet in data:  \n",
    "            df = self.pd.DataFrame(tweet, columns=['topic_num', 'probability'])\n",
    "            topics.append(df.loc[df['probability'].idxmax(), 'topic_num'])\n",
    "        return (self.pd.Series(topics))\n",
    "    \n",
    "    def get_sentiment(self,text_column):\n",
    "        from textblob import TextBlob\n",
    "        return text_column.apply(lambda x: TextBlob(' '.join(x)).sentiment.polarity)\n",
    "    \n",
    "    def get_length_of_comment(self,text_column):\n",
    "        return text_column.apply(lambda x: len(x))\n",
    "    \n",
    "    def amount_of_upper(self,text_column):\n",
    "        return text_column.apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "    \n",
    "    def create_uppercase_max_sequence_column(self,text_column):\n",
    "        def get_max_uppercase_run_from_string(s):\n",
    "            lengths=[len(x) for x in self.re.findall(r\"[A-Z]+\", s)]\n",
    "            if len(lengths)>0:\n",
    "                return max(lengths)\n",
    "            else:\n",
    "                return 0\n",
    "        return (text_column.apply(lambda x: get_max_uppercase_run_from_string(x)))\n",
    "    \n",
    "    def amount_of_sign(self,text_column,sign):\n",
    "        return text_column.apply(lambda x: sum(1 for c in x if c==sign))\n",
    "    \n",
    "    def max_sequence(self,text_column,symbol):\n",
    "        def get_max_sequence_run_from_string(s,symbol):\n",
    "            lengths=[len(x) for x in self.re.findall(r\"[\"+symbol+\"]+\", s)]\n",
    "            if len(lengths)>0:\n",
    "                return max(lengths)\n",
    "            else:\n",
    "                return 0   \n",
    "        return text_column.apply(lambda x: get_max_sequence_run_from_string(x,symbol))\n",
    "    \n",
    "    def remove_punctuation(self,text_column):\n",
    "        import string\n",
    "        return text_column.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "    \n",
    "    def bad_comments_column_bin(self,text_column,bad_words_list):\n",
    "        def is_comment_bad(comment,bad_words_list):\n",
    "            for word in comment:\n",
    "                if word in bad_words_list:\n",
    "                    return 1\n",
    "            return 0\n",
    "        return text_column.apply(lambda x: is_comment_bad(x,bad_words_list))\n",
    "\n",
    "    def bad_comments_column_agg(self,text_column,bad_words_list):\n",
    "        def amount_comment_bad_words(comment,bad_words_list):\n",
    "            count_of_bad_words=0\n",
    "            for word in comment:\n",
    "                if word in bad_words_list:\n",
    "                    count_of_bad_words=count_of_bad_words+1\n",
    "            return count_of_bad_words\n",
    "        return text_column.apply(lambda x: amount_comment_bad_words(x,bad_words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp V.0.1 \n",
      "Imported pandas,re,nltk,numpy packages\n"
     ]
    }
   ],
   "source": [
    "nlp=nlp()\n",
    "X=pd.DataFrame()\n",
    "train_test_text=train.comment_text.append(test.comment_text,ignore_index=True)\n",
    "X['length']=nlp.get_length_of_comment(train_test_text)\n",
    "X['amount_upper']=nlp.amount_of_upper(train_test_text)\n",
    "X['max_upper']=nlp.create_uppercase_max_sequence_column(train_test_text)\n",
    "X['sentiment']=nlp.get_sentiment(train_test_text)\n",
    "# X['amount_question_marks']=nlp.amount_of_sign(train_test_text,'?')\n",
    "X['amount_exclamations']=nlp.amount_of_sign(train_test_text,'!')\n",
    "X['amount_stars']=nlp.amount_of_sign(train_test_text,'*')\n",
    "X['amount_dashes']=nlp.amount_of_sign(train_test_text,'-')\n",
    "X['amount_equal_signs']=nlp.amount_of_sign(train_test_text,'=')\n",
    "# X['max_seq_question_marks']=nlp.max_sequence(train_test_text,'?')\n",
    "X['max_seq_exclamations']=nlp.max_sequence(train_test_text,'!')\n",
    "X['max_seq_stars']=nlp.max_sequence(train_test_text,'*')\n",
    "X['max_seq_dashes']=nlp.max_sequence(train_test_text,'-')\n",
    "train_test_text=nlp.remove_punctuation(train_test_text)\n",
    "train_test_text=nlp.clean_text_for_topic_modelling(train_test_text)\n",
    "train_test_text=nlp.remove_stopwords_from_corpus(train_test_text)\n",
    "# train_test_text=nlp.lemmatize_corpus(train_test_text)\n",
    "train_test_text=nlp.stemm_corpus(train_test_text,stemmer='snowball')\n",
    "train_test_text=nlp.create_ngrams(train_test_text,num_grams=2)\n",
    "bad_words=pd.read_csv('bad_words.csv',names=['bad_words'])\n",
    "bad_words=nlp.stemm_corpus(bad_words,stemmer='snowball')\n",
    "bad_words=list(bad_words['bad_words'])\n",
    "X['bad_words_bin']=nlp.bad_comments_column_bin(train_test_text,bad_words)\n",
    "X['bad_words_agg']=nlp.bad_comments_column_agg(train_test_text,bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic=nlp.gensim_dic(train_test_text,no_below=11,no_above=0.8)\n",
    "bow_corp=nlp.bow_corpus(dic,train_test_text)\n",
    "tfidf=nlp.vec_to_tfidf(bow_corp)\n",
    "lda_model = nlp.lda_model(tfidf,dic)\n",
    "X['lda_topic'] = nlp.assign_lda_topic(lda_model,bow_corp)\n",
    "tfidf_df=nlp.tf_idf_tomatrix(dic,tfidf)\n",
    "scaler = StandardScaler()\n",
    "X=pd.DataFrame(scaler.fit_transform(X),columns=X.columns)\n",
    "X=X.join(tfidf_df)\n",
    "X_train=X.iloc[:10000].copy()\n",
    "X_test=X.iloc[10000:].copy()\n",
    "y_toxic=train['toxic'].copy()\n",
    "y_obscene=train['obscene'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9084678344069671"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2, X_validate, y_train, y_validate = train_test_split(X_train, y_obscene, test_size=0.25, random_state=22)\n",
    "clf = LogisticRegression(penalty='l1',solver='liblinear',C=0.75)\n",
    "clf.fit(X_train_2, y_train)\n",
    "f1_score(y_validate, clf.predict(X_validate), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_validate, y_train, y_validate = train_test_split(X_train, y_obscene, test_size=0.25, random_state=4)\n",
    "rf = RandomForestClassifier(max_depth=100, random_state=0,n_estimators=1000)\n",
    "rf.fit(X_train_2, y_train)\n",
    "f1_score(y_validate, rf.predict(X_validate), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_toxic)\n",
    "toxic_results=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_obscene)\n",
    "obscene_results=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try14=test.join(pd.DataFrame(toxic_results,columns=['toxic']))\n",
    "try14=try14.join(pd.DataFrame(obscene_results,columns=['obscene']))\n",
    "try14.drop('comment_text',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try14.to_csv('try14.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs=pd.DataFrame(clf.coef_,columns=X.columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
